# generated by fastapi-codegen:
#   filename:  openapi_modified.yaml
#   timestamp: 2024-05-04T14:14:42+00:00

from __future__ import annotations

import copy
import time
import uuid
from typing import TypedDict

import tritonserver
from fastapi import FastAPI
from vllm.transformers_utils.tokenizer import get_tokenizer

triton_server = tritonserver.Server(
    model_repository="/workspace/llm-models", log_verbose=6, strict_model_config=False
).start(wait_until_ready=True)

model = triton_server.model("llama-3-8b-instruct")

model.tokenizer = get_tokenizer(tokenizer_name="meta-llama/Meta-Llama-3-8B-Instruct")

from models import (
    ChatCompletionResponseMessage,
    Choice,
    Choice1,
    CreateChatCompletionRequest,
    CreateChatCompletionResponse,
    CreateCompletionRequest,
    CreateCompletionResponse,
    DeleteModelResponse,
    FinishReason,
    FinishReason1,
    ListModelsResponse,
    Logprobs,
    Logprobs2,
    Model,
    Object1,
    Object2,
    Role5,
)

app = FastAPI(
    title="OpenAI API",
    description="The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.",
    version="2.0.0",
    termsOfService="https://openai.com/policies/terms-of-use",
    contact={"name": "OpenAI Support", "url": "https://help.openai.com/"},
    license={
        "name": "MIT",
        "url": "https://github.com/openai/openai-openapi/blob/master/LICENSE",
    },
    servers=[{"url": "https://api.openai.com/v1"}],
)


class ConversationMessage(TypedDict):
    role: str
    content: str


@app.post(
    "/chat/completions", response_model=CreateChatCompletionResponse, tags=["Chat"]
)
def create_chat_completion(
    body: CreateChatCompletionRequest,
) -> CreateChatCompletionResponse:
    """
    Creates a model response for the given chat conversation.
    """

    conversation = [
        ConversationMessage(
            role=str(message.dict()["role"]), content=str(message.dict()["content"])
        )
        for message in body.messages
    ]

    prompt = model.tokenizer.apply_chat_template(
        conversation=conversation, tokenize=False, add_generation_prompt=False
    )

    exclude_input_in_output = True

    parameters = copy.deepcopy(body.dict())
    if "prompt" in parameters:
        del parameters["prompt"]
    if "stream" in parameters:
        del parameters["stream"]
    if "echo" in parameters:
        del parameters["echo"]
    if "model" in parameters:
        del parameters["model"]
    if "messages" in parameters:
        del parameters["messages"]

    response = list(
        model.infer(
            inputs={
                "text_input": [prompt],
                "stream": [False],
                "exclude_input_in_output": [exclude_input_in_output],
            },
            parameters=parameters,
        )
    )[0]

    return CreateChatCompletionResponse(
        id="foo",
        choices=[
            Choice1(
                finish_reason=FinishReason1.stop,
                index=0,
                message=ChatCompletionResponseMessage(
                    content=response.outputs["text_output"].to_string_array()[0],
                    role=Role5.assistant,
                    function_call=None,
                ),
                logprobs=Logprobs2(content=[]),
            )
        ],
        created=0,
        model="foo",
        system_fingerprint=None,
        object=Object2.chat_completion,
    )


@app.post("/completions", response_model=CreateCompletionResponse, tags=["Completions"])
def create_completion(body: CreateCompletionRequest) -> CreateCompletionResponse:
    """
    Creates a completion for the provided prompt and parameters.
    """
    exclude_input_in_output = True

    if body.echo:
        exclude_input_in_output = False

    parameters = copy.deepcopy(body.dict())
    del parameters["prompt"]
    del parameters["stream"]
    del parameters["echo"]
    del parameters["model"]

    response = list(
        model.infer(
            inputs={
                "text_input": [body.prompt],
                "stream": [False],
                "exclude_input_in_output": [exclude_input_in_output],
            },
            parameters=parameters,
        )
    )[0]

    choice = Choice(
        finish_reason=FinishReason.stop,
        index=0,
        logprobs=Logprobs(),
        text=response.outputs["text_output"].to_string_array()[0],
    )

    return CreateCompletionResponse(
        id=f"cmpl-{uuid.uuid1()}",
        created=int(time.time()),
        model=model.name,
        choices=[choice],
        system_fingerprint=None,
        object=Object1.text_completion,
    )


@app.get("/models", response_model=ListModelsResponse, tags=["Models"])
def list_models() -> ListModelsResponse:
    """
    Lists the currently available models, and provides basic information about each one such as the owner and availability.
    """
    pass


@app.get("/models/{model}", response_model=Model, tags=["Models"])
def retrieve_model(model: str) -> Model:
    """
    Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
    """
    pass


@app.delete("/models/{model}", response_model=DeleteModelResponse, tags=["Models"])
def delete_model(model: str) -> DeleteModelResponse:
    """
    Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.
    """
    pass
